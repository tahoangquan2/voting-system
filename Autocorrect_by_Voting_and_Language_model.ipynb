{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bVqOa8MAD4D"
      },
      "source": [
        "#Install package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0KpyTgMWa_L"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install surya-ocr\n",
        "!pip install paddlepaddle paddleocr openpyxl\n",
        "!pip install shapely\n",
        "!pip install easyocr\n",
        "!pip install paddleocr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk8lSQhy6rvX"
      },
      "source": [
        "#Initialize image path and output path\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tahoangquan2/voting-system.git"
      ],
      "metadata": {
        "id": "_Ipdzku-892G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaCP_AJs6q7L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "image_folder = '/content/voting-system/input'\n",
        "output_folder= '/content/voting-system/output'\n",
        "# os.makedirs(output_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q43AbybU3q3-"
      },
      "source": [
        "#Collect PaddleOCR Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lStwkdQz6hhd"
      },
      "outputs": [],
      "source": [
        "from paddleocr import PaddleOCR, draw_ocr\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvCJ4gb-61f0"
      },
      "outputs": [],
      "source": [
        "# Initialize the PaddleOCR detector\n",
        "ocr = PaddleOCR(use_angle_cls=True, lang='en', use_gpu = True)  # Adjust language as needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9QOPqX362Qi"
      },
      "outputs": [],
      "source": [
        "# List and sort images based on extracted page number\n",
        "images_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.jpeg', '.jpg', '.png', '.bmp'))]\n",
        "\n",
        "\n",
        "# Process images and save results\n",
        "results = []\n",
        "\n",
        "for idx, image_name in enumerate(images_files):\n",
        "    image_path = os.path.join(image_folder, image_name)\n",
        "\n",
        "    # Perform OCR detection\n",
        "    ocr_result = ocr.ocr(image_path, cls=False)\n",
        "\n",
        "    if ocr_result[0]:\n",
        "    # Collect bounding box data and confidence\n",
        "      for line in ocr_result[0]:\n",
        "          box = line[0]  # Bounding box coordinates\n",
        "          confidence = line[1][1]  # Confidence score\n",
        "          results.append({\n",
        "              \"Page Number\": idx + 1,\n",
        "              \"Bounding box\": box,\n",
        "              \"Confidence\": confidence  # Add confidence score\n",
        "          })\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "Paddledf = pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysA6O2EM3zcq"
      },
      "source": [
        "#Collect VietOCR Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AM8sMD3c8Q8E"
      },
      "outputs": [],
      "source": [
        "cd '/content/voting-system/vietnamese-ocr'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn4xGo1pZpNx"
      },
      "outputs": [],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FlioUZj8hMs"
      },
      "outputs": [],
      "source": [
        "!pip install -r /content/voting-system/vietnamese-ocr/requirement.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Chl28Fr8koJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# from PaddleOCR import PaddleOCR, draw_ocr\n",
        "\n",
        "\n",
        "# Now import modules\n",
        "from vietocr.vietocr.tool.predictor import Predictor\n",
        "from vietocr.vietocr.tool.config import Cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJIHxb9r8r7F"
      },
      "outputs": [],
      "source": [
        "# Configure of VietOCR\n",
        "config = Cfg.load_config_from_name('vgg_transformer')\n",
        "# config = Cfg.load_config_from_file('vietocr/config.yml')\n",
        "# config['weights'] = '/Users/bmd1905/Desktop/pretrain_ocr/vi00_vi01_transformer.pth'\n",
        "\n",
        "config['cnn']['pretrained'] = True\n",
        "config['predictor']['beamsearch'] = True\n",
        "config['device'] = 'cuda:0' # mps\n",
        "\n",
        "VietOCRrecognitor = Predictor(config)\n",
        "\n",
        "# Example Usage\n",
        "# img = cv2.imread(img_path)\n",
        "# img = Image.fromarray(img)\n",
        "# rec_result = VietOCRrecognitor.predict(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTR956luCCuO"
      },
      "outputs": [],
      "source": [
        "%cd ../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEt_oXki34qF"
      },
      "source": [
        "#Collect Surya Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKXsdq7T3754"
      },
      "outputs": [],
      "source": [
        "from surya.detection import batch_text_detection\n",
        "from surya.model.detection.model import load_model as load_det_model, load_processor as load_det_processor\n",
        "\n",
        "from surya.recognition import batch_recognition\n",
        "from surya.model.recognition.model import load_model\n",
        "from surya.model.recognition.processor import load_processor\n",
        "\n",
        "\n",
        "model, processor = load_det_model(), load_det_processor()\n",
        "recognition_model, recognition_processor = load_model(), load_processor()\n",
        "langs = [\"vi\"]  # Replace with your languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esVAtnwK-lqT"
      },
      "outputs": [],
      "source": [
        "def Surya_line_det(images_list):\n",
        "    bbox_list = []\n",
        "\n",
        "    for idx, image in enumerate(images_list):\n",
        "        print(f'Processing {image}...')\n",
        "        image_path = os.path.join(image_folder, image)\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        predictions = batch_text_detection([image], model, processor)\n",
        "        for bbox in predictions[0].bboxes:\n",
        "            bbox_list.append({\n",
        "                \"Page Number\": idx + 1,\n",
        "                \"Bounding box\": bbox.polygon,\n",
        "                \"Confidence\": bbox.confidence\n",
        "            })\n",
        "\n",
        "    return bbox_list\n",
        "\n",
        "\n",
        "def Surya_ocr(warp_image):\n",
        "    recognition, _ = batch_recognition([warp_image], [langs], recognition_model, recognition_processor)\n",
        "    # print(f\"[!] {image_name} done\")\n",
        "    return recognition[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwPmL5b3_ndn"
      },
      "outputs": [],
      "source": [
        "# List and sort images based on extracted page number\n",
        "images_list = [f for f in os.listdir(image_folder) if f.lower().endswith(('.jpeg', '.jpg', '.png', '.bmp'))]\n",
        "bbox_list = Surya_line_det(images_list)\n",
        "Suryabbdf = pd.DataFrame(bbox_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh0VjDlN38md"
      },
      "source": [
        "#Collect EasyOCR Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSb87odB3yWN"
      },
      "outputs": [],
      "source": [
        "import easyocr\n",
        "reader = easyocr.Reader(['vi'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caBhx39QJF5t"
      },
      "outputs": [],
      "source": [
        "def easyocr_ocr(warp_image):\n",
        "    ocr_result = reader.recognize(warp_image)\n",
        "    return ocr_result[0][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPlqJ_sf4Dc1"
      },
      "source": [
        "#Voting on text detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0b-dUFIDOZI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from shapely.geometry import Polygon\n",
        "\n",
        "def calculate_overlap_ratio(box1, box2):\n",
        "    try:\n",
        "        polygon1 = Polygon(box1)\n",
        "        polygon2 = Polygon(box2)\n",
        "\n",
        "        if not polygon1.is_valid or not polygon2.is_valid:\n",
        "            return 0.0\n",
        "\n",
        "        intersection = polygon1.intersection(polygon2)\n",
        "\n",
        "        area1 = polygon1.area\n",
        "        area2 = polygon2.area\n",
        "        intersection_area = intersection.area\n",
        "\n",
        "        ratio1 = intersection_area / area1 if area1 > 0 else 0.0\n",
        "        ratio2 = intersection_area / area2 if area2 > 0 else 0.0\n",
        "\n",
        "        return max(ratio1, ratio2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating overlap: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def parse_bbox(bbox_str):\n",
        "\n",
        "    return ast.literal_eval(str(bbox_str))\n",
        "\n",
        "def compare_ocr_results(df1, df2):\n",
        "    output_data = []\n",
        "\n",
        "    for page in df1['Page Number'].unique():\n",
        "        page_boxes1 = df1[df1['Page Number'] == page]\n",
        "        page_boxes2 = df2[df2['Page Number'] == page]\n",
        "        matched_boxes2 = set()\n",
        "\n",
        "        for _, box1 in page_boxes1.iterrows():\n",
        "            # print(box1['Bounding box'])\n",
        "            box1_coords = parse_bbox(box1['Bounding box'])\n",
        "            matched = False\n",
        "\n",
        "            for idx2, box2 in page_boxes2.iterrows():\n",
        "                if idx2 in matched_boxes2:\n",
        "                    continue\n",
        "\n",
        "                box2_coords = parse_bbox(box2['Bounding box'])\n",
        "                overlap_ratio = calculate_overlap_ratio(box1_coords, box2_coords)\n",
        "\n",
        "                if overlap_ratio >= 0.4:\n",
        "                    avg_confidence = (box1['Confidence'] + box2['Confidence']) / 2\n",
        "                    if avg_confidence > 0.5:\n",
        "                        output_data.append({\n",
        "                            'Page Number': page,\n",
        "                            'Bounding box': box1['Bounding box'],\n",
        "                            'Confidence': avg_confidence,\n",
        "                            'Source': 'matched'\n",
        "                        })\n",
        "                    matched_boxes2.add(idx2)\n",
        "                    matched = True\n",
        "\n",
        "            if not matched and box1['Confidence'] > 0.8:\n",
        "                output_data.append({\n",
        "                    'Page Number': page,\n",
        "                    'Bounding Box': box1['Bounding Box'],\n",
        "                    'Confidence': box1['Confidence'],\n",
        "                    'Source': 'ocr1'\n",
        "                })\n",
        "\n",
        "        for idx2, box2 in page_boxes2.iterrows():\n",
        "            if idx2 not in matched_boxes2 and box2['Confidence'] > 0.8:\n",
        "                output_data.append({\n",
        "                    'Page Number': page,\n",
        "                    'Bounding Box': box2['Bounding Box'],\n",
        "                    'Confidence': box2['Confidence'],\n",
        "                    'Source': 'ocr2'\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(output_data)\n",
        "\n",
        "\n",
        "matched_results = compare_ocr_results(Paddledf, Suryabbdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooUzcaUO4L1w"
      },
      "source": [
        "#Image Warp Perspective"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34-mkDMr4KtQ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import ast\n",
        "from itertools import groupby\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "def order_points(pts):\n",
        "    rect = np.zeros((4, 2), dtype=\"float32\")\n",
        "    s = pts.sum(axis=1)\n",
        "    rect[0] = pts[np.argmin(s)]\n",
        "    rect[2] = pts[np.argmax(s)]\n",
        "    diff = np.diff(pts, axis=1)\n",
        "    rect[1] = pts[np.argmin(diff)]\n",
        "    rect[3] = pts[np.argmax(diff)]\n",
        "    return rect\n",
        "\n",
        "def four_point_transform(image, pts):\n",
        "    rect = order_points(pts)\n",
        "    (tl, tr, br, bl) = rect\n",
        "\n",
        "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
        "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
        "    maxWidth = max(int(widthA), int(widthB))\n",
        "\n",
        "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
        "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
        "    maxHeight = max(int(heightA), int(heightB))\n",
        "\n",
        "    padding = 4\n",
        "    padded_width = maxWidth + (2 * padding)\n",
        "    padded_height = maxHeight + (2 * padding)\n",
        "\n",
        "    dst = np.array([\n",
        "        [padding, padding],\n",
        "        [padded_width - padding - 1, padding],\n",
        "        [padded_width - padding - 1, padded_height - padding - 1],\n",
        "        [padding, padded_height - padding - 1]], dtype=\"float32\")\n",
        "\n",
        "    M = cv2.getPerspectiveTransform(rect, dst)\n",
        "    warped = cv2.warpPerspective(image, M, (padded_width, padded_height))\n",
        "    return warped\n",
        "\n",
        "def process_images_with_boxes(image_files, df):\n",
        "    \"\"\"\n",
        "    Process a list of image files with bounding boxes from DataFrame\n",
        "\n",
        "    Args:\n",
        "        image_files: List of image file paths\n",
        "        df: DataFrame with bounding box information\n",
        "\n",
        "    Returns:\n",
        "        List of lists, where each inner list contains the cropped regions for one image\n",
        "    \"\"\"\n",
        "    all_regions = []  # List to store lists of regions for each image\n",
        "\n",
        "    # Process each image\n",
        "    for page_idx, image_path in enumerate(image_files):\n",
        "        page_num = page_idx + 1  # Assuming page numbers start from 1\n",
        "\n",
        "        # Get boxes for current page\n",
        "        page_boxes = df[df['Page Number'] == page_num]\n",
        "\n",
        "        # Read the image\n",
        "        image_path = os.path.join(image_folder, image_path)\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print(f\"Error: Could not read image: {image_path}\")\n",
        "            all_regions.append([])  # Add empty list for failed image\n",
        "            continue\n",
        "\n",
        "        # List to store regions for current image\n",
        "        page_regions = []\n",
        "\n",
        "        # Process each box in the current page\n",
        "        for _, row in page_boxes.iterrows():\n",
        "            try:\n",
        "                bbox = np.array(ast.literal_eval(str(row['Bounding box'])), dtype=\"float32\")\n",
        "                warped = four_point_transform(image, bbox)\n",
        "                page_regions.append(warped)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing region in page {page_num}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        all_regions.append(page_regions)\n",
        "\n",
        "    return all_regions\n",
        "\n",
        "processed_regions = process_images_with_boxes(images_files, matched_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVpZ9jKAo62f"
      },
      "source": [
        "# Draw bounding box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSoJegV-pArt"
      },
      "outputs": [],
      "source": [
        "def draw_bounding_boxes(images_files, df):\n",
        "    \"\"\"\n",
        "    Draw bounding boxes on images and save them to output folder\n",
        "\n",
        "    Args:\n",
        "        images_files: List of image file paths\n",
        "        df: DataFrame with bounding box information\n",
        "    \"\"\"\n",
        "    # Create output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    for page_idx, image_path in enumerate(images_files):\n",
        "        page_num = page_idx + 1\n",
        "\n",
        "        # Get boxes for current page\n",
        "        page_boxes = df[df['Page Number'] == page_num]\n",
        "\n",
        "        # Read original image\n",
        "        image = cv2.imread(os.path.join(image_folder, image_path))\n",
        "        if image is None:\n",
        "            print(f\"Error: Could not read image: {image_path}\")\n",
        "            continue\n",
        "\n",
        "        # Draw all bounding boxes\n",
        "        for _, row in page_boxes.iterrows():\n",
        "            bbox = np.array(ast.literal_eval(str(row['Bounding box'])), dtype=\"float32\")\n",
        "            # Draw lines connecting the points\n",
        "            for i in range(4):\n",
        "                pt1 = tuple(bbox[i])\n",
        "                pt2 = tuple(bbox[(i + 1) % 4])\n",
        "                cv2.line(image,\n",
        "                        (int(pt1[0]), int(pt1[1])),\n",
        "                        (int(pt2[0]), int(pt2[1])),\n",
        "                        (255, 0, 0), 2)  # Blue color, thickness=2\n",
        "\n",
        "        # Save the image\n",
        "        output_path = os.path.join(output_folder, f'bbox_{os.path.basename(image_path)}')\n",
        "        cv2.imwrite(output_path, image)\n",
        "        print(f\"Saved image with bounding boxes to: {output_path}\")\n",
        "\n",
        "draw_bounding_boxes(images_files, matched_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFTscC5E4P08"
      },
      "source": [
        "#Voting on text recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cja5YcOOBLGO"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "# Expanded mapping of accents to their corresponding numbers\n",
        "ACCENT_TO_NUMBER = {\n",
        "    \"á\": 1, \"à\": 2, \"ã\": 3, \"ả\": 4, \"ạ\": 5,\n",
        "    \"é\": 1, \"è\": 2, \"ẽ\": 3, \"ẻ\": 4, \"ẹ\": 5,\n",
        "    \"í\": 1, \"ì\": 2, \"ĩ\": 3, \"ỉ\": 4, \"ị\": 5,\n",
        "    \"ó\": 1, \"ò\": 2, \"õ\": 3, \"ỏ\": 4, \"ọ\": 5,\n",
        "    \"ú\": 1, \"ù\": 2, \"ũ\": 3, \"ủ\": 4, \"ụ\": 5,\n",
        "    \"ý\": 1, \"ỳ\": 2, \"ỹ\": 3, \"ỷ\": 4, \"ỵ\": 5,\n",
        "    \"ấ\": 1, \"ầ\": 2, \"ẫ\": 3, \"ẩ\": 4, \"ậ\": 5,\n",
        "    \"ế\": 1, \"ề\": 2, \"ễ\": 3, \"ể\": 4, \"ệ\": 5,\n",
        "    \"ố\": 1, \"ồ\": 2, \"ỗ\": 3, \"ổ\": 4, \"ộ\": 5,\n",
        "    \"ắ\": 1, \"ằ\": 2, \"ẵ\": 3, \"ẳ\": 4, \"ặ\": 5,\n",
        "    \"ớ\": 1, \"ờ\": 2, \"ỡ\": 3, \"ở\": 4, \"ợ\": 5,\n",
        "    \"ứ\": 1, \"ừ\": 2, \"ữ\": 3, \"ử\": 4, \"ự\": 5#,\n",
        "    #\"đ\": 9  # For 'đ', assigning a unique number\n",
        "}\n",
        "\n",
        "# Mapping of accented characters to their base (unmarked) characters\n",
        "ACCENT_TO_BASE = {\n",
        "    \"á\": \"a\", \"à\": \"a\", \"ã\": \"a\", \"ả\": \"a\", \"ạ\": \"a\",\n",
        "    \"é\": \"e\", \"è\": \"e\", \"ẽ\": \"e\", \"ẻ\": \"e\", \"ẹ\": \"e\",\n",
        "    \"í\": \"i\", \"ì\": \"i\", \"ĩ\": \"i\", \"ỉ\": \"i\", \"ị\": \"i\",\n",
        "    \"ó\": \"o\", \"ò\": \"o\", \"õ\": \"o\", \"ỏ\": \"o\", \"ọ\": \"o\",\n",
        "    \"ú\": \"u\", \"ù\": \"u\", \"ũ\": \"u\", \"ủ\": \"u\", \"ụ\": \"u\",\n",
        "    \"ý\": \"y\", \"ỳ\": \"y\", \"ỹ\": \"y\", \"ỷ\": \"y\", \"ỵ\": \"y\",\n",
        "    \"â\": \"â\", \"ấ\": \"â\", \"ầ\": \"â\", \"ẫ\": \"â\", \"ẩ\": \"â\", \"ậ\": \"â\",\n",
        "    \"ê\": \"ê\", \"ế\": \"ê\", \"ề\": \"ê\", \"ễ\": \"ê\", \"ể\": \"ê\", \"ệ\": \"ê\",\n",
        "    \"ô\": \"o\", \"ố\": \"o\", \"ồ\": \"o\", \"ỗ\": \"o\", \"ổ\": \"o\", \"ộ\": \"o\",\n",
        "    \"ă\": \"ă\", \"ắ\": \"ă\", \"ằ\": \"ă\", \"ẵ\": \"ă\", \"ẳ\": \"ă\", \"ặ\": \"ă\",\n",
        "    \"ơ\": \"ơ\", \"ớ\": \"ơ\", \"ờ\": \"ơ\", \"ỡ\": \"ơ\", \"ở\": \"ơ\", \"ợ\": \"ơ\",\n",
        "    \"ư\": \"ư\", \"ứ\": \"ư\", \"ừ\": \"ư\", \"ữ\": \"ư\", \"ử\": \"ư\", \"ự\": \"ư\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def normalize_vietnamese(word):\n",
        "    normalized_word = \"\"\n",
        "    accent_number = \"\"\n",
        "    word = word.lower()\n",
        "    for char in word:\n",
        "        if char in ACCENT_TO_NUMBER:\n",
        "            normalized_word += ACCENT_TO_BASE[char]  # Replace with the base character\n",
        "            accent_number = str(ACCENT_TO_NUMBER[char])  # Set the corresponding accent number\n",
        "        else:\n",
        "            normalized_word += char  # Add the character as is if no accent\n",
        "\n",
        "    return normalized_word + accent_number\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import difflib\n",
        "\n",
        "def load_words(file_path):\n",
        "    \"\"\"Load words from the given text file.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return [line.strip() for line in file]\n",
        "\n",
        "def load_normalize_words(file_path):\n",
        "    \"\"\"Load words from the given text file.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return [normalize_vietnamese(line.strip()) for line in file]\n",
        "\n",
        "def find_similar_words(word, word_list, max_results=10):\n",
        "    \"\"\"\n",
        "    Find similar words using difflib.\n",
        "    - Returns up to `max_results` similar words from the word list.\n",
        "    \"\"\"\n",
        "    similar_list  = []\n",
        "    normalize_word = normalize_vietnamese(word)\n",
        "    cnt = 0\n",
        "    for check in word_list:\n",
        "        normalize_check = normalize_vietnamese(check)\n",
        "        similar, dis = words_similar(normalize_word, normalize_check, threshold= 0.3)\n",
        "\n",
        "        if similar:\n",
        "            similar_list.append(check)\n",
        "            cnt += 1\n",
        "\n",
        "        if(cnt >= max_results):\n",
        "            break\n",
        "    return similar_list\n",
        "\n",
        "\n",
        "\n",
        "def check_word_in_file(word, vietnam_word_list, normalize_vietnam_list):\n",
        "    \"\"\"\n",
        "    Check if a word is in the file and return suggestions if it's not found.\n",
        "    - If found, return \"Yes\".\n",
        "    - If not, return \"No\" and a list of similar words.\n",
        "    \"\"\"\n",
        "\n",
        "    normalize_word = normalize_vietnamese(word)\n",
        "    normalize_word = re.sub(r'[.,:_;?/\"()…]', '', normalize_word)\n",
        "    if normalize_word in normalize_vietnam_list:\n",
        "        similar_words = [word]\n",
        "        return True, similar_words\n",
        "    else:\n",
        "        similar_words = find_similar_words(word, vietnam_word_list)\n",
        "        return False, similar_words\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    # Remove newline characters\n",
        "    text = re.sub(r'\\s*[\\n\\r]+\\s*', ' ', text)\n",
        "\n",
        "    # Convert \"~\", \"=\", \"—\", \"→\", \"_\" to \"-\"\n",
        "    text = text.replace(\"~\", \"-\")\n",
        "    text = text.replace(\"=\", \"-\")\n",
        "    text = text.replace(\"—\", \"-\")\n",
        "    text = text.replace(\"→\", '-')\n",
        "    text = text.replace(\"_\", '-')\n",
        "\n",
        "    # Convert \"«»\", \"<>\" to '\"\"'\n",
        "    text = text.replace(\"« \", '\"')\n",
        "    text = text.replace(\" »\", '\"')\n",
        "    text = text.replace(\"< \", '\"')\n",
        "    text = text.replace(\" >\", '\"')\n",
        "\n",
        "    # Convert '...' to '…'\n",
        "    text = re.sub(r'\\.\\.\\.', '…', text)\n",
        "\n",
        "    # Ensure punctuation is followed by a space\n",
        "    text = re.sub(r'([….,;:?!])\\s', r'\\1 ', text)\n",
        "\n",
        "    # Remove spaces before punctuation\n",
        "    text = re.sub(r'\\s+([….,!?;:])', r'\\1', text)\n",
        "\n",
        "    # Ensure only one space between words\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Ensure a space follows \"-\" at the start of the text\n",
        "    text = re.sub(r'^-\\s*', '- ', text)\n",
        "\n",
        "    # Strip leading and trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    # Return normalized text\n",
        "    return text\n",
        "\n",
        "def normalize_word(word):\n",
        "    \"\"\"\n",
        "    Normalize a word to lowercase and remove accents (diacritical marks).\n",
        "\n",
        "    Args:\n",
        "        word (str): The word to normalize.\n",
        "\n",
        "    Returns:\n",
        "        str: The normalized word.\n",
        "    \"\"\"\n",
        "\n",
        "    if(word ==  'None'):\n",
        "        return word\n",
        "    # Convert to lowercase\n",
        "    word = word.lower()\n",
        "\n",
        "    # Remove accents by decomposing the Unicode characters and filtering\n",
        "    word = ''.join(\n",
        "        char for char in unicodedata.normalize('NFD', word)\n",
        "        if unicodedata.category(char) != 'Mn'\n",
        "    )\n",
        "    word = re.sub(r'-', '', word)\n",
        "    word = re.sub(r'f', 't', word)\n",
        "    word = re.sub(r'j', 'i', word)\n",
        "\n",
        "\n",
        "    return word\n",
        "\n",
        "\n",
        "def levenshtein_distance(a, b):\n",
        "    \"\"\"Calculate the Levenshtein distance between two strings.\"\"\"\n",
        "    len_a, len_b = len(a), len(b)\n",
        "    dp = [[0] * (len_b + 1) for _ in range(len_a + 1)]\n",
        "\n",
        "    # Initialize base cases\n",
        "    for i in range(len_a + 1):\n",
        "        dp[i][0] = i  # Cost of deleting all characters from `a`\n",
        "    for j in range(len_b + 1):\n",
        "        dp[0][j] = j  # Cost of inserting all characters into `a`\n",
        "\n",
        "    # Compute distances\n",
        "    for i in range(1, len_a + 1):\n",
        "        for j in range(1, len_b + 1):\n",
        "            if a[i - 1] == b[j - 1]:\n",
        "                cost = 0  # No cost if characters match\n",
        "            else:\n",
        "                cost = 1  # Substitution cost\n",
        "            dp[i][j] = min(\n",
        "                dp[i - 1][j] + 1,      # Deletion\n",
        "                dp[i][j - 1] + 1,      # Insertion\n",
        "                dp[i - 1][j - 1] + cost  # Substitution\n",
        "            )\n",
        "\n",
        "    return dp[len_a][len_b]\n",
        "\n",
        "\n",
        "def words_similar(word1, word2, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Check if two words are similar based on Levenshtein distance\n",
        "    after normalization.\n",
        "\n",
        "    Args:\n",
        "        word1 (str): First word.\n",
        "        word2 (str): Second word.\n",
        "        threshold (float): Maximum allowed distance as a fraction of word length.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the words are similar enough, False otherwise.\n",
        "    \"\"\"\n",
        "    if(word1 == 'None' and word1 == word2):\n",
        "        return True, 4\n",
        "    if(word1 == 'None'):\n",
        "        return False, len(word2)\n",
        "    if(word2 == 'None'):\n",
        "        return False, len(word1)\n",
        "    # Normalize both words\n",
        "    word1 = normalize_word(word1)\n",
        "    word2 = normalize_word(word2)\n",
        "    # Calculate Levenshtein distance\n",
        "    distance = levenshtein_distance(word1, word2)\n",
        "    if(distance <= threshold * max(len(word2), len(word1))):\n",
        "      return True , distance\n",
        "    else:\n",
        "      return False , distance\n",
        "\n",
        "\n",
        "def MED_to_word(sen1, sen2):\n",
        "    # Initialize cache for MED calculation and operations tracking\n",
        "    sen1 = sen1.strip()\n",
        "    sen2 = sen2.strip()\n",
        "    seq1 = sen1.split(' ')\n",
        "    seq2 = sen2.split(' ')\n",
        "    cache = [[float(\"inf\")] * (len(seq2) + 1) for _ in range(len(seq1) + 1)]\n",
        "    ops = [[None] * (len(seq2) + 1) for _ in range(len(seq1) + 1)]\n",
        "\n",
        "    # Fill base cases\n",
        "    for j in range(len(seq2) + 1):\n",
        "        cache[len(seq1)][j] = len(seq2) - j\n",
        "        ops[len(seq1)][j] = 'insert'\n",
        "    for i in range(len(seq1) + 1):\n",
        "        cache[i][len(seq2)] = len(seq1) - i\n",
        "        ops[i][len(seq2)] = 'delete'\n",
        "\n",
        "    # Fill the cache and ops table\n",
        "    for i in range(len(seq1) - 1, -1, -1):\n",
        "        for j in range(len(seq2) - 1, -1, -1):\n",
        "            compare, dis = words_similar(seq1[i],seq2[j])\n",
        "            if compare:\n",
        "                cache[i][j] = cache[i + 1][j + 1]\n",
        "                ops[i][j] = 'match'  # Characters match, move diagonally\n",
        "            else:\n",
        "                # Consider all operations: insert, delete, substitute\n",
        "                insert_cost = 1 + cache[i][j + 1]\n",
        "                delete_cost = 1 + cache[i + 1][j]\n",
        "                substitute_cost = 1 + cache[i + 1][j + 1]\n",
        "\n",
        "                # Choose the operation with the minimum cost\n",
        "                if insert_cost <= delete_cost and insert_cost <= substitute_cost:\n",
        "                    cache[i][j] = insert_cost\n",
        "                    ops[i][j] = 'insert'\n",
        "                elif delete_cost <= insert_cost and delete_cost <= substitute_cost:\n",
        "                    cache[i][j] = delete_cost\n",
        "                    ops[i][j] = 'delete'\n",
        "                else:\n",
        "                    cache[i][j] = substitute_cost\n",
        "                    ops[i][j] = 'substitute'\n",
        "\n",
        "    # Backtrack\n",
        "    aligned_seq1, aligned_seq2 = [], []\n",
        "    i, j = 0, 0\n",
        "    while i < len(seq1) or j < len(seq2):\n",
        "        if i < len(seq1) and j < len(seq2) and ops[i][j] == 'match':\n",
        "            aligned_seq1.append(seq1[i])\n",
        "            aligned_seq2.append(seq2[j])\n",
        "            i += 1\n",
        "            j += 1\n",
        "        elif i < len(seq1) and ops[i][j] == 'delete':\n",
        "            aligned_seq1.append(seq1[i])\n",
        "            aligned_seq2.append('None')\n",
        "            i += 1\n",
        "        elif j < len(seq2) and ops[i][j] == 'insert':\n",
        "            aligned_seq1.append('None')\n",
        "            aligned_seq2.append(seq2[j])\n",
        "            j += 1\n",
        "        elif i < len(seq1) and j < len(seq2) and ops[i][j] == 'substitute':\n",
        "            aligned_seq1.append(seq1[i])\n",
        "            aligned_seq2.append(seq2[j])\n",
        "            i += 1\n",
        "            j += 1\n",
        "\n",
        "    aligned_seq1 = ' '.join(aligned_seq1)\n",
        "    aligned_seq2 = ' '.join(aligned_seq2)\n",
        "    return aligned_seq1, aligned_seq2\n",
        "\n",
        "\n",
        "def align_multiple_sequences(sequences, best_ocr = 1):\n",
        "    cnt = len(sequences)\n",
        "    try_case = 0\n",
        "    while try_case < len(sequences):\n",
        "        aligned_sequences = sequences[:]\n",
        "        limit = 0\n",
        "        base_sequence = aligned_sequences.pop(try_case)\n",
        "        while limit < 10:\n",
        "            # Align each remaining sequence to the base sequence\n",
        "            updated_sequences = []\n",
        "            for seq in aligned_sequences:\n",
        "                aligned_seq, base_sequence = MED_to_word(seq, base_sequence)\n",
        "                updated_sequences.append(aligned_seq)\n",
        "\n",
        "            # Update the list of aligned sequences\n",
        "            aligned_sequences = updated_sequences\n",
        "\n",
        "            # Check if all sequences are aligned to the same length\n",
        "\n",
        "\n",
        "            if all(len(seq. split(' ')) == len(base_sequence.split(' ')) for seq in aligned_sequences):\n",
        "                aligned_sequences.append(base_sequence)\n",
        "                return aligned_sequences, 0\n",
        "\n",
        "\n",
        "\n",
        "            limit += 1\n",
        "\n",
        "        try_case += 1\n",
        "\n",
        "\n",
        "    # If can't align return the best ocr result\n",
        "    res = []\n",
        "    for i in range(cnt):\n",
        "        res.append(sequences[best_ocr])\n",
        "    return res, 1\n",
        "\n",
        "\n",
        "def read_ocr_inputs(csv_file):\n",
        "    ocr_outputs = []\n",
        "    with open(csv_file, mode='r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        for row in reader:\n",
        "            # Skip the filename column and extract the next 4 columns\n",
        "            ocr_outputs.append(row[1:5])  # Columns 2 to 5 (0-based index)\n",
        "\n",
        "    return ocr_outputs\n",
        "\n",
        "\n",
        "import string\n",
        "\n",
        "def most_common_end_punctuation(strings):\n",
        "    \"\"\"\n",
        "    Find the most common ending punctuation in a list of strings.\n",
        "    :param strings: List of strings\n",
        "    :return: The most common ending punctuation and its count\n",
        "    \"\"\"\n",
        "    end_punctuations = []\n",
        "\n",
        "    # Define punctuation characters\n",
        "    punctuation_set = {'?', '!', '.', ';' , ',', ']', '}', '%' , '$', '…'}\n",
        "\n",
        "    # Collect ending punctuation from each string\n",
        "    for s in strings:\n",
        "        if s and s[-1] in punctuation_set:\n",
        "            end_punctuations.append(s[-1])\n",
        "\n",
        "    # Use Counter to find the most common ending punctuation\n",
        "    if end_punctuations:\n",
        "        counter = Counter(end_punctuations)\n",
        "        return counter.most_common(1)[0][0]  # Return the most common punctuation and its count\n",
        "    else:\n",
        "        return '' # No ending punctuation found\n",
        "\n",
        "def most_common_start_punctuation(strings):\n",
        "    \"\"\"\n",
        "    Find the most common start punctuation in a list of strings.\n",
        "    :param strings: List of strings\n",
        "    :return: The most common start punctuation and its count\n",
        "    \"\"\"\n",
        "    start_punctuations = []\n",
        "\n",
        "    # Define punctuation characters\n",
        "    punctuation_set = ['-', '\"', '(' , '{', '[']\n",
        "\n",
        "    # Collect ending punctuation from each string\n",
        "    for s in strings:\n",
        "        if s and s[0] in punctuation_set:\n",
        "            start_punctuations.append(s[0])\n",
        "\n",
        "    # Use Counter to find the most common ending punctuation\n",
        "    if start_punctuations:\n",
        "        counter = Counter(start_punctuations)\n",
        "        return counter.most_common(1)[0][0]  # Return the most common punctuation and its count\n",
        "    else:\n",
        "        return '' # No ending punctuation found\n",
        "\n",
        "def is_integer(s):\n",
        "    \"\"\"\n",
        "    Check if a string represents an integer.\n",
        "    :param s: Input string\n",
        "    :return: True if the string represents an integer, False otherwise\n",
        "    \"\"\"\n",
        "    try:\n",
        "        int(s)  # Try converting the string to an integer\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def vote(list_word, vietnam_word_list, normalize_vietnam_list):\n",
        "\n",
        "    vote_container = []\n",
        "    weight_container = []\n",
        "    begin_punctuation = most_common_start_punctuation(list_word)\n",
        "    end_punctuation = most_common_end_punctuation(list_word)\n",
        "\n",
        "    for i in range(len(list_word)): #Early termination if there is a valid word and a word same to it\n",
        "        i = len(list_word) - 1 - i\n",
        "        nw = re.sub(r'[.,:_;?/\"()…]', '', list_word[i])\n",
        "        if(nw == '' or nw == 'None'):\n",
        "            continue\n",
        "        in_list, similar_words = check_word_in_file(nw, vietnam_word_list, normalize_vietnam_list)\n",
        "        if in_list:\n",
        "            for j in range(len(list_word)):\n",
        "                if(i != j):\n",
        "                    now = re.sub(r'[.,:_;?/\"()…]', '', list_word[j])\n",
        "                    if(now == nw):\n",
        "                        return list_word[i], 2004\n",
        "\n",
        "    for i , word in enumerate(reversed(list_word)):\n",
        "        org_word = word\n",
        "        if word == 'None' or word == '':\n",
        "            continue\n",
        "        word = re.sub(r'[.,:_;?/\"()…]', '', word)\n",
        "        if is_integer(word):\n",
        "            vote_container.append(word)\n",
        "            weight_container.append(1)\n",
        "            continue\n",
        "        in_list, similar_words = check_word_in_file(word, vietnam_word_list, normalize_vietnam_list)\n",
        "        if in_list:\n",
        "            for other_word in list_word:\n",
        "                if org_word != other_word:  # Avoid comparing the same element\n",
        "                    other_word = re.sub(r'[.,:_;?/\"(){}[]…]', '', other_word)\n",
        "                    nw = normalize_vietnamese(normalize_word(word))\n",
        "                    now = normalize_vietnamese(normalize_word(other_word))\n",
        "                    similar, dist = words_similar(nw, now, threshold = 0.3)\n",
        "                    if(similar): #Early termination if there is a valid word and a word similar to it\n",
        "                            return begin_punctuation + word + end_punctuation, 2011\n",
        "\n",
        "            vote_container.append(word)\n",
        "            weight_container.append(1 * (1.25 * (i // 2)))\n",
        "            continue\n",
        "        for sim in similar_words:\n",
        "            vote_container.append(sim)\n",
        "            dist = levenshtein_distance(sim, word)\n",
        "            weight_container.append(0.68 * (1 - dist/max(len(word), len(sim))) * (1.25 * (i // 2)))\n",
        "\n",
        "    if not vote_container:\n",
        "        if begin_punctuation == '' and end_punctuation == '':\n",
        "            return None, 0  # Return None if the list is empty\n",
        "        else:\n",
        "            return begin_punctuation + end_punctuation, 1\n",
        "\n",
        "    if len(vote_container) != len(weight_container):\n",
        "        raise ValueError(\"vote_container and weight_container must have the same length.\")\n",
        "\n",
        "        # Aggregate weights for each unique vote\n",
        "    weight_sum = defaultdict(float)\n",
        "    for vote, weight in zip(vote_container, weight_container):\n",
        "        weight_sum[vote] += weight\n",
        "\n",
        "    # Find the item with the highest total weight\n",
        "    most_common = max(weight_sum.items(), key=lambda item: item[1])\n",
        "    most_common_item, total_weight = most_common\n",
        "\n",
        "    return begin_punctuation + most_common_item + end_punctuation , total_weight\n",
        "\n",
        "\n",
        "def read_and_vote(warp_images_list, vietnam_word_list, normalize_vietnam_list ):\n",
        "\n",
        "    # for bbox in df[\"Bounding Box\"]:  # Select by column label\n",
        "\n",
        "    # Iterate through rows starting from the second row\n",
        "    rowID = 1\n",
        "    image_list = []\n",
        "    for image_id, warp_images in enumerate(warp_images_list):\n",
        "          line_list = []\n",
        "          for warp_image in warp_images:\n",
        "\n",
        "              pil_img = Image.fromarray(warp_image)\n",
        "              sentence1 = VietOCRrecognitor.predict(pil_img) # VietOCR text\n",
        "              sentence2 = easyocr_ocr(warp_image) # Easy OCR text\n",
        "              sentence3 = Surya_ocr(pil_img)   # Surya text\n",
        "\n",
        "\n",
        "              sequences = [sentence1, sentence2, sentence3]\n",
        "              align_sentences, success = align_multiple_sequences(sequences)\n",
        "              vote_sentence_splits = []\n",
        "\n",
        "              if success == '1':\n",
        "                  vote_sentence = align_sentences[0]\n",
        "              else:\n",
        "                  sentences_splits = []\n",
        "                  for align in align_sentences:\n",
        "                      word_list = align.split(' ')\n",
        "                      sentences_splits.append(word_list)\n",
        "                  for i in range(len(sentences_splits[0])):\n",
        "                      word1 = sentences_splits[0][i]\n",
        "                      word2 = sentences_splits[1][i]\n",
        "                      word3 = sentences_splits[2][i]\n",
        "\n",
        "                      list_word_to_vote = [word1, word2, word3]\n",
        "                      vote_word = vote(list_word_to_vote, vietnam_word_list, normalize_vietnam_list)\n",
        "\n",
        "                      if vote_word[0]:\n",
        "                          vote_sentence_splits.append(vote_word[0])\n",
        "\n",
        "                  vote_sentence = ' '.join(vote_sentence_splits)\n",
        "                  print(vote_sentence)\n",
        "                  line_list.append(vote_sentence)\n",
        "          image_list.append(line_list)\n",
        "    return image_list\n",
        "\n",
        "          # Writing the voted text and bounding box to a text file\n",
        "          # Assuming warp_image contains the image ID and bounding boxes in some form\n",
        "          #image_id = warp_image[\"image_id\"]  # Replace with actual image ID retrieval\n",
        "          # bounding_boxes = warp_image[\"bounding_boxes\"]  # Replace with actual bounding box retrieval\n",
        "\n",
        "          # label_file_path = os.path.join(output_folder, f\"{image_id}.txt\")\n",
        "          # with open(label_file_path, 'w') as label_file:\n",
        "          #     for i, bbox in enumerate(bounding_boxes):\n",
        "          #         # bbox is assumed to be a tuple or list (x_min, y_min, x_max, y_max)\n",
        "          #         label_line = f\"{bbox} {vote_sentence}\\n\"\n",
        "          #         label_file.write(label_line)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "vietnamese_file_path = '/content/voting-system/voting/VN_MorphoSyllable_List.txt'\n",
        "vietnam_word_list = load_words(vietnamese_file_path)\n",
        "normalize_vietnam_list = load_normalize_words(vietnamese_file_path)\n",
        "voted_ocr = read_and_vote(processed_regions, vietnam_word_list, normalize_vietnam_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQnIe1lm4V6M"
      },
      "source": [
        "#Apply Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dps0r6ll4Yfq"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "corrector = pipeline(\"text2text-generation\", model=\"bmd1905/vietnamese-correction-v2\", device=0)\n",
        "MAX_LENGTH = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6iP1yGVO6jE"
      },
      "outputs": [],
      "source": [
        "def revert_pipeline_changes(ocr_lines, predictions):\n",
        "    check_list = []\n",
        "    reverted_lines = []\n",
        "\n",
        "    for row, (ocr_line, prediction) in enumerate(zip(ocr_lines, predictions)):\n",
        "        pred_text = prediction['generated_text']\n",
        "\n",
        "        # Remove added capitalization if OCR doesn't have it\n",
        "        if ocr_line and pred_text[0].isupper() and ocr_line[0].islower():\n",
        "            pred_text = pred_text[0].lower() + pred_text[1:]\n",
        "\n",
        "        # Remove added punctuation if OCR doesn't have it\n",
        "        punctuation_marks = [\".\", \"!\", \"?\", \":\", \"…\", \",\"]\n",
        "        for mark in punctuation_marks:\n",
        "            if ocr_line and not ocr_line.endswith(mark) and pred_text.endswith(mark):\n",
        "                check_list.append((row + 2, 1))\n",
        "                pred_text = pred_text[:-1]\n",
        "                break\n",
        "\n",
        "        ocr_word = ocr_line.split()\n",
        "        pred_word = pred_text.split()\n",
        "        if len(pred_word) - len(ocr_word) > 1:\n",
        "            check_list.append((row + 2, 2))\n",
        "            pred_text = \" \".join(pred_word[:len(ocr_word)])\n",
        "        elif len(pred_word) - len(ocr_word) == 1 and levenshtein_distance(pred_word[1], ocr_word[0])/len(ocr_word[0]) < 0.5:\n",
        "            check_list.append((row + 2, 3))\n",
        "            pred_text = \" \".join(pred_word[1:])\n",
        "        elif len(pred_word) - len(ocr_word) == 1:\n",
        "            check_list.append((row + 2, 4))\n",
        "            pred_text = \" \".join(pred_word[:-1])\n",
        "\n",
        "        reverted_lines.append(pred_text)\n",
        "\n",
        "    return reverted_lines, check_list\n",
        "\n",
        "\n",
        "def batch_predictions(voted_ocr):\n",
        "    predictions = corrector(voted_ocr, max_length=MAX_LENGTH)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4SRXIjSPqaO"
      },
      "outputs": [],
      "source": [
        "LM_results = []\n",
        "for image in voted_ocr:\n",
        "  predictions = batch_predictions(image)\n",
        "  refine_ocr, check_list = revert_pipeline_changes(image, predictions)\n",
        "  LM_results.append(refine_ocr)\n",
        "\n",
        "print(LM_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Output"
      ],
      "metadata": {
        "id": "xaCh8RT6uw6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def write_results_to_file(images_files, matched_results, text_strings, output_file):\n",
        "    \"\"\"\n",
        "    Write results to a text file in the specified format\n",
        "\n",
        "    Args:\n",
        "        images_files: List of image file paths\n",
        "        matched_results: DataFrame with Page Number and Bounding box\n",
        "        text_strings: Texts corresponding to the bounding boxes\n",
        "        output_file: Path to output text file\n",
        "    \"\"\"\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        # Process each page\n",
        "        for page_idx, image_path in enumerate(images_files):\n",
        "            page_num = page_idx + 1\n",
        "\n",
        "            # Write image path\n",
        "            f.write(f\"output_images/{image_path}\")\n",
        "\n",
        "            # Get boxes and texts for current page\n",
        "            page_data = matched_results[matched_results['Page Number'] == page_num]\n",
        "\n",
        "            # Create list for JSON data\n",
        "            json_data = []\n",
        "\n",
        "            # Add each box and text pair\n",
        "            for idx, row in page_data.iterrows():\n",
        "                # Convert bounding box string to list of points\n",
        "                bbox = np.array(ast.literal_eval(str(row['Bounding box'])), dtype=\"float32\")\n",
        "                points = [[int(x), int(y)] for x, y in bbox]\n",
        "\n",
        "                # Get corresponding text\n",
        "                text = text_strings[page_idx][len(json_data)]  # Use current count as index\n",
        "\n",
        "                # Create entry\n",
        "                entry = {\n",
        "                    \"transcription\": text,\n",
        "                    \"points\": points,\n",
        "                    \"difficult\": False\n",
        "                }\n",
        "                json_data.append(entry)\n",
        "\n",
        "            # Write JSON data\n",
        "            f.write(\" \" + json.dumps(json_data, ensure_ascii=False))\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "output_file = output_folder + \"/LM.txt\"\n",
        "write_results_to_file(images_files, matched_results, LM_results, output_file)\n",
        "\n",
        "output_file = output_folder + \"/Vote.txt\"\n",
        "write_results_to_file(images_files, matched_results, voted_ocr, output_file)"
      ],
      "metadata": {
        "id": "BR8EtbziuzjY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}